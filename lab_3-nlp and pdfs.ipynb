{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Procesing \n",
    "### Example from a pdf\n",
    "\n",
    "\n",
    "Note (possible consideration for projects) -you do not need to work with a pdf for nlp!    \n",
    "e.g. work with a .txt file, read off of a web page etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes for HW2   \n",
    "\n",
    "You need to make your code abstract enough to work for future docs. So if I wanted to run this code for 2017 I should not have to change more than one line of code. You don't need to verify that you get the correct answers with other years, just generalize your code enough so that it runs for other years \n",
    "     \n",
    "Good code orients users to what they'll need to change up at the top\n",
    "\n",
    "You need to use functions in this assignment. (and going forward)\n",
    "\n",
    "It is ok to hard code the page numbers      \n",
    "Best practice:     \n",
    "- If you need to hard code, make it an argument\n",
    "\n",
    "You'll want to try to catch negations (e.g. \"will not increase\", \"failed to rise\") \n",
    "\n",
    "Tip:\n",
    "Writing fns: \n",
    "- Think of what it should do, and mock it out\n",
    "- Write example input\n",
    "- Try again with different input\n",
    "- Use print statements to tell you what's happening\n",
    "- Remove or comment out the intermediary output when finished\n",
    "\n",
    "#Spacy documentation\n",
    "#https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements belong at the top of your code\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2 \n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") #English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://countyofsb.org/ceo/asset.c/4171'\n",
    "filename = 'FY_2020_21_Section_B_Executive_Summary.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a comment for where this function is called\n",
    "# e.g. called in main()\n",
    "# but for this example, I'm calling my fns imediately (to demonstrate)\n",
    "def get_pdf(url, filename, path):\n",
    "    response = requests.get(url)\n",
    "    with open(os.path.join(path, filename), 'wb') as ofile:\n",
    "        ofile.write(response.content)\n",
    "\n",
    "\n",
    "\n",
    "if filename not in os.listdir():\n",
    "    print('downloading document from {}'.format(url))\n",
    "    get_pdf(url, filename, path)\n",
    "else:\n",
    "    print('document already in {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filename, path):\n",
    "    with open(os.path.join(path, filename), 'rb') as ifile:\n",
    "        pdf = PyPDF2.PdfFileReader(ifile)\n",
    "\n",
    "        print('Number of pages:', pdf.numPages)\n",
    "\n",
    "        pages = []\n",
    "        for p in range(pdf.numPages):\n",
    "            page = pdf.getPage(p)\n",
    "            text = page.extractText()\n",
    "            text = text.replace(\"â„¢\", \"'\")\n",
    "            text = text.replace(\"\\n\", \"\")\n",
    "            pages.append(text)\n",
    "        \n",
    "        return pages\n",
    "\n",
    "pages = read_pdf(filename, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[2][0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(pages, page_num):\n",
    "    text = pages[page_num]\n",
    "    doc = nlp(text)\n",
    "    return doc\n",
    "\n",
    "tokenized_page = tokenize(pages, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_page[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenized_page[50].ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenized_page[50].children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring our page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_terms = ['pandemic', 'COVID']\n",
    "covid_tokens = [t for t in tokenized_page if any([e in t.string for e in covid_terms])]\n",
    "covid_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_ancs = [list(t.ancestors) for t in covid_tokens]\n",
    "covid_ancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested for loop\n",
    "for ancs in covid_ancs:\n",
    "    for anc in ancs:\n",
    "        print(anc, anc.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_anc_type = [[(anc, anc.pos_) for anc in ancs] for ancs in covid_ancs]\n",
    "covid_anc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_ancs_verbs = [[a for a in ancs if a.pos_ == 'VERB'] for ancs in covid_ancs]\n",
    "covid_ancs_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_list in econ_ancestors:\n",
    "    for ancestor in token_list:\n",
    "        print(ancestor.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_children = [list(t.children) for t in covid_tokens]\n",
    "covid_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_of_covid_ancs = [[list(a.children) for a in ancs] for ancs in covid_ancs]\n",
    "children_of_covid_ancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenized_page[0:10].noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenized_page[10:100].noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(tokenized_page[45:47].noun_chunks)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0].root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_nchunks = [nc for nc in tokenized_page.noun_chunks if 'budget' in nc.string]\n",
    "budget_nchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_ancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_list in covid_ancs:\n",
    "    #print(t_list) #debug\n",
    "    for token in t_list:\n",
    "        #print(token) #debug\n",
    "        if str(token) == 'accelerated':\n",
    "            print('ancestor', list(token.ancestors))\n",
    "            print('child', list(token.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_list in covid_ancs:\n",
    "    for token in t_list:\n",
    "        if str(token) == 'accelerated':\n",
    "            accelerated_anc = list(token.ancestors) # expect \"is\"\n",
    "\n",
    "for token in accelerated_anc:\n",
    "    print(list(token.ancestors))\n",
    "    print(list(token.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This can all be deeply unsatisfying, and you're going to hit a lot of dead ends. Sometimes you do just have to use a brute-force approach though. \n",
    "Try enough things and you'll get what you're looking for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take look at just one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The COVID19 pandemic has caused a national recession'\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, list(token.ancestors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, list(token.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = spacy.displacy.render(doc, style='dep', options={'distance' : 140}, jupyter=False)\n",
    "with open('dependency parser.svg', 'w', encoding='utf-8') as f:\n",
    "    f.write(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtree pulls the token, its ancestors and its children\n",
    "for token in doc:\n",
    "    print(token, list(token.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = list(doc.noun_chunks)\n",
    "nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc[0].root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc[1].root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
